{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Let's fit two linear models, one for classification and one for regression.\n",
    "\n",
    "1. Breast cancer wisconsin dataset  (classification). https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer \n",
    "\n",
    "`from sklearn.datasets import load_breast_cancer\n",
    "X_bc, y_bc = load_breast_cancer(return_X_y=True)`\n",
    "\n",
    "\n",
    "2. California housing dataset (regression) https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing\n",
    "\n",
    "`from sklearn.datasets import fetch_california_housing\n",
    "X_cal, y_cal = fetch_california_housing(return_X_y=True)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Use train_test_split to create two subsets of data, one for fitting the model and the other for testing the model (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) hint:\n",
    "`from sklearn.model_selection import train_test_split`\n",
    "\n",
    "* Fit one model for each dataset. test two different values of parameters apart of the parameter by default. Check the score on the test set. hint: `from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression`\n",
    "\n",
    "* Hint: use ? e.g. `LogisticRegression?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Let's fit two models one tree based and one SVC.\n",
    "\n",
    "Use the wine dataset from scikit-learn. Similar to the previous hands-on exercises, split the data and run using different parameters to obtain the best score on the test set (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html).\n",
    "\n",
    "`from sklearn.datasets import load_wine\n",
    "X_w , y_w = load_wine(return_X_y=True)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and datasets\n",
    "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer Wisconsin dataset\n",
    "#df_bc = load_breast_cancer()\n",
    "X_bc, y_bc = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Load the California Housing dataset\n",
    "X_cal, y_cal = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "#print (X_bc, y_bc)\n",
    "#print (X_cal, y_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n"
     ]
    }
   ],
   "source": [
    "# Split the Breast Cancer dataset into training and testing sets for classification\n",
    "X_bc_train, X_bc_test, y_bc_train, y_bc_test = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the California Housing dataset into training and testing sets for regression\n",
    "X_cal_train, X_cal_test, y_cal_train, y_cal_test = train_test_split(X_cal, y_cal, test_size=0.2, random_state=42)\n",
    "\n",
    "print (X_bc_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (C=0.01): Accuracy = 0.96\n",
      "Logistic Regression (C=1.0): Accuracy = 0.97\n"
     ]
    }
   ],
   "source": [
    "# Classification with Logistic Regression\n",
    "# Fit the Logistic Regression model with different values of C (inverse regularization strength)\n",
    "C_values = [0.01, 1.0]\n",
    "for C in C_values:\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_bc_train_scaled = scaler.fit_transform(X_bc_train)\n",
    "    X_bc_test_scaled = scaler.transform(X_bc_test)\n",
    "    \n",
    "    clf = LogisticRegression(C=C, max_iter=2000, solver='saga', random_state=0)  # Increase max_iter value and change solver\n",
    "    clf.fit(X_bc_train_scaled, y_bc_train)\n",
    "    y_pred_bc = clf.predict(X_bc_test_scaled)\n",
    "    accuracy = accuracy_score(y_bc_test, y_pred_bc)\n",
    "    \n",
    "    print(f\"Logistic Regression (C={C}): Accuracy = {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "The logistic regression model with a lower value of the regularization parameter (C=0.1) achieved a slightly higher accuracy of 0.98 compared to the model with a higher value of C (C=1.0) which had an accuracy of 0.97. This suggests that the model with weaker regularization (lower C) may have performed slightly better in correctly classifying the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: Mean Squared Error = 0.56, R^2 Score = 0.58\n",
      "Ridge Regression (alpha=10): Mean Squared Error = 0.56, R^2 Score = 0.58\n",
      "Ridge Regression (alpha=1.0): Mean Squared Error = 0.56, R^2 Score = 0.58\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "# Fit the Linear Regression model with different values of alpha (not applicable to Linear Regression)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_cal_train, y_cal_train)\n",
    "y_pred_lr = lr.predict(X_cal_test)\n",
    "mse_lr = mean_squared_error(y_cal_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_cal_test, y_pred_lr)\n",
    "print(f\"Linear Regression: Mean Squared Error = {mse_lr:.2f}, R^2 Score = {r2_lr:.2f}\")\n",
    "\n",
    "# Ridge Regression\n",
    "# Fit the Ridge Regression model with different values of alpha\n",
    "alpha_values = [10, 1.0]\n",
    "best_r2_ridge = -1  # Initialize with a negative value\n",
    "best_alpha = None\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_cal_train, y_cal_train)\n",
    "    y_pred_ridge = ridge.predict(X_cal_test)\n",
    "    mse_ridge = mean_squared_error(y_cal_test, y_pred_ridge)\n",
    "    r2_ridge = r2_score(y_cal_test, y_pred_ridge)\n",
    "    \n",
    "    print(f\"Ridge Regression (alpha={alpha}): Mean Squared Error = {mse_ridge:.2f}, R^2 Score = {r2_ridge:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "All three regression models, Linear Regression, and the two Ridge Regression variations with different alpha values, produce nearly identical results in terms of Mean Squared Error (MSE) and R-squared (R^2) score. The MSE is 0.56 for all three models, indicating the same level of error in predictions. Similarly, the R^2 score is 0.58 for all models, implying that these models explain a consistent amount of variance in the target variable.\n",
    "\n",
    "This suggests that, in this specific case, the choice of alpha for Ridge Regression does not significantly impact the model's performance, and the simple Linear Regression model performs equally well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset\n",
    "X_w, y_w = load_wine(return_X_y=True)\n",
    "\n",
    "# Split the Wine dataset into training and testing sets\n",
    "X_w_train, X_w_test, y_w_train, y_w_test = train_test_split(X_w, y_w, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree model:\n",
      "Max Depth: 4\n",
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Tree-based model - Decision Tree\n",
    "# Fit the Decision Tree model with different values of max_depth\n",
    "max_depth_values = [2, 4, 6, 8, 10]\n",
    "best_accuracy_tree = 0\n",
    "best_max_depth_tree = 0\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    tree_classifier = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\n",
    "    tree_classifier.fit(X_w_train, y_w_train)\n",
    "    y_pred_tree = tree_classifier.predict(X_w_test)\n",
    "    accuracy_tree = accuracy_score(y_w_test, y_pred_tree)\n",
    "    \n",
    "    if accuracy_tree > best_accuracy_tree:\n",
    "        best_accuracy_tree = accuracy_tree\n",
    "        best_max_depth_tree = max_depth\n",
    "\n",
    "print(f\"Best Decision Tree model:\")\n",
    "print(f\"Max Depth: {best_max_depth_tree}\")\n",
    "print(f\"Accuracy: {best_accuracy_tree:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVC model:\n",
      "C: 1.0\n",
      "Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Classifier (SVC)\n",
    "# Fit the SVC model with different values of C (regularization parameter)\n",
    "C_values = [0.1, 1.0, 10.0]\n",
    "best_accuracy_svc = 0\n",
    "best_C_svc = 0\n",
    "\n",
    "for C in C_values:\n",
    "    svc_classifier = SVC(C=C, random_state=0)\n",
    "    svc_classifier.fit(X_w_train, y_w_train)\n",
    "    y_pred_svc = svc_classifier.predict(X_w_test)\n",
    "    accuracy_svc = accuracy_score(y_w_test, y_pred_svc)\n",
    "    \n",
    "    if accuracy_svc > best_accuracy_svc:\n",
    "        best_accuracy_svc = accuracy_svc\n",
    "        best_C_svc = C\n",
    "\n",
    "print(f\"Best SVC model:\")\n",
    "print(f\"C: {best_C_svc}\")\n",
    "print(f\"Accuracy: {best_accuracy_svc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "The Decision Tree model with a maximum depth of 4 achieves a significantly higher accuracy of 0.94, indicating strong performance in correctly classifying the data. In contrast, the Support Vector Classifier (SVC) with a C value of 1.0 has a lower accuracy of 0.81, suggesting it is not as effective at classifying the data in this specific case.\n",
    "\n",
    "In this comparison, the Decision Tree model outperforms the SVC model in terms of accuracy, making it the better choice for this classification task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
